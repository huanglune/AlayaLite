/*
 * Copyright 2025 VectorDB.NTU
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#pragma once

namespace alaya {
// Fast Hadamard Transform using AVX instructions
// NOLINTBEGIN

inline void helper_float_6(float *buf) {
#if defined(__AVX512F__)
  for (int j = 0; j < 64; j += 64) {
    for (int k = 0; k < 8; k += 8) {
      __asm__ volatile(
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $160, %%ymm2, %%ymm8\n"
          "vpermilps $245, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilps $160, %%ymm3, %%ymm8\n"
          "vpermilps $245, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilps $160, %%ymm4, %%ymm8\n"
          "vpermilps $245, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilps $160, %%ymm5, %%ymm8\n"
          "vpermilps $245, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilps $160, %%ymm6, %%ymm8\n"
          "vpermilps $245, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilps $160, %%ymm7, %%ymm8\n"
          "vpermilps $245, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vpermilps $68, %%ymm2, %%ymm8\n"
          "vpermilps $238, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm2\n"
          "vpermilps $68, %%ymm3, %%ymm8\n"
          "vpermilps $238, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm3\n"
          "vpermilps $68, %%ymm4, %%ymm8\n"
          "vpermilps $238, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm4\n"
          "vpermilps $68, %%ymm5, %%ymm8\n"
          "vpermilps $238, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm5\n"
          "vpermilps $68, %%ymm6, %%ymm8\n"
          "vpermilps $238, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm6\n"
          "vpermilps $68, %%ymm7, %%ymm8\n"
          "vpermilps $238, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm7\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm2, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm2\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm3, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm3\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm4, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm4\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm5, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm5\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm6, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm6\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm7, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n" ::"r"(buf + j + k + 0),
          "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32),
          "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56)
          : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8",
            "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
    }
  }
#endif
}

inline void helper_float_7(float *buf) {
#if defined(__AVX512F__)
  for (int j = 0; j < 128; j += 64) {
    for (int k = 0; k < 8; k += 8) {
      __asm__ volatile(
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $160, %%ymm2, %%ymm8\n"
          "vpermilps $245, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilps $160, %%ymm3, %%ymm8\n"
          "vpermilps $245, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilps $160, %%ymm4, %%ymm8\n"
          "vpermilps $245, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilps $160, %%ymm5, %%ymm8\n"
          "vpermilps $245, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilps $160, %%ymm6, %%ymm8\n"
          "vpermilps $245, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilps $160, %%ymm7, %%ymm8\n"
          "vpermilps $245, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vpermilps $68, %%ymm2, %%ymm8\n"
          "vpermilps $238, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm2\n"
          "vpermilps $68, %%ymm3, %%ymm8\n"
          "vpermilps $238, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm3\n"
          "vpermilps $68, %%ymm4, %%ymm8\n"
          "vpermilps $238, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm4\n"
          "vpermilps $68, %%ymm5, %%ymm8\n"
          "vpermilps $238, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm5\n"
          "vpermilps $68, %%ymm6, %%ymm8\n"
          "vpermilps $238, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm6\n"
          "vpermilps $68, %%ymm7, %%ymm8\n"
          "vpermilps $238, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm7\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm2, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm2\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm3, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm3\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm4, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm4\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm5, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm5\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm6, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm6\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm7, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n" ::"r"(buf + j + k + 0),
          "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32),
          "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56)
          : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8",
            "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
    }
  }
  for (int j = 0; j < 128; j += 128) {
    for (int k = 0; k < 64; k += 8) {
      __asm__ volatile(
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n" ::"r"(buf + j + k + 0),
          "r"(buf + j + k + 64)
          : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8",
            "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
    }
  }
#endif
}

inline void helper_float_8_recursive(float *buf, int depth) {
#if defined(__AVX512F__)
  if (depth == 6) {
    for (int j = 0; j < 64; j += 64) {
      for (int k = 0; k < 8; k += 8) {
        __asm__ volatile(
            "vmovups (%0), %%ymm0\n"
            "vmovups (%1), %%ymm1\n"
            "vmovups (%2), %%ymm2\n"
            "vmovups (%3), %%ymm3\n"
            "vmovups (%4), %%ymm4\n"
            "vmovups (%5), %%ymm5\n"
            "vmovups (%6), %%ymm6\n"
            "vmovups (%7), %%ymm7\n"
            "vpermilps $160, %%ymm0, %%ymm8\n"
            "vpermilps $245, %%ymm0, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
            "vpermilps $160, %%ymm1, %%ymm8\n"
            "vpermilps $245, %%ymm1, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
            "vpermilps $160, %%ymm2, %%ymm8\n"
            "vpermilps $245, %%ymm2, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
            "vpermilps $160, %%ymm3, %%ymm8\n"
            "vpermilps $245, %%ymm3, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
            "vpermilps $160, %%ymm4, %%ymm8\n"
            "vpermilps $245, %%ymm4, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
            "vpermilps $160, %%ymm5, %%ymm8\n"
            "vpermilps $245, %%ymm5, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
            "vpermilps $160, %%ymm6, %%ymm8\n"
            "vpermilps $245, %%ymm6, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
            "vpermilps $160, %%ymm7, %%ymm8\n"
            "vpermilps $245, %%ymm7, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
            "vpermilps $68, %%ymm0, %%ymm8\n"
            "vpermilps $238, %%ymm0, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm0\n"
            "vpermilps $68, %%ymm1, %%ymm8\n"
            "vpermilps $238, %%ymm1, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm1\n"
            "vpermilps $68, %%ymm2, %%ymm8\n"
            "vpermilps $238, %%ymm2, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm2\n"
            "vpermilps $68, %%ymm3, %%ymm8\n"
            "vpermilps $238, %%ymm3, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm3\n"
            "vpermilps $68, %%ymm4, %%ymm8\n"
            "vpermilps $238, %%ymm4, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm4\n"
            "vpermilps $68, %%ymm5, %%ymm8\n"
            "vpermilps $238, %%ymm5, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm5\n"
            "vpermilps $68, %%ymm6, %%ymm8\n"
            "vpermilps $238, %%ymm6, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm6\n"
            "vpermilps $68, %%ymm7, %%ymm8\n"
            "vpermilps $238, %%ymm7, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm7\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm0, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm0\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm1, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm1\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm2, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm2\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm3, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm3\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm4, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm4\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm5, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm5\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm6, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm6\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm7, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm7\n"
            "vaddps %%ymm1, %%ymm0, %%ymm8\n"
            "vsubps %%ymm1, %%ymm0, %%ymm9\n"
            "vaddps %%ymm3, %%ymm2, %%ymm10\n"
            "vsubps %%ymm3, %%ymm2, %%ymm11\n"
            "vaddps %%ymm5, %%ymm4, %%ymm12\n"
            "vsubps %%ymm5, %%ymm4, %%ymm13\n"
            "vaddps %%ymm7, %%ymm6, %%ymm14\n"
            "vsubps %%ymm7, %%ymm6, %%ymm15\n"
            "vaddps %%ymm10, %%ymm8, %%ymm0\n"
            "vsubps %%ymm10, %%ymm8, %%ymm2\n"
            "vaddps %%ymm11, %%ymm9, %%ymm1\n"
            "vsubps %%ymm11, %%ymm9, %%ymm3\n"
            "vaddps %%ymm14, %%ymm12, %%ymm4\n"
            "vsubps %%ymm14, %%ymm12, %%ymm6\n"
            "vaddps %%ymm15, %%ymm13, %%ymm5\n"
            "vsubps %%ymm15, %%ymm13, %%ymm7\n"
            "vaddps %%ymm4, %%ymm0, %%ymm8\n"
            "vsubps %%ymm4, %%ymm0, %%ymm12\n"
            "vaddps %%ymm5, %%ymm1, %%ymm9\n"
            "vsubps %%ymm5, %%ymm1, %%ymm13\n"
            "vaddps %%ymm6, %%ymm2, %%ymm10\n"
            "vsubps %%ymm6, %%ymm2, %%ymm14\n"
            "vaddps %%ymm7, %%ymm3, %%ymm11\n"
            "vsubps %%ymm7, %%ymm3, %%ymm15\n"
            "vmovups %%ymm8, (%0)\n"
            "vmovups %%ymm9, (%1)\n"
            "vmovups %%ymm10, (%2)\n"
            "vmovups %%ymm11, (%3)\n"
            "vmovups %%ymm12, (%4)\n"
            "vmovups %%ymm13, (%5)\n"
            "vmovups %%ymm14, (%6)\n"
            "vmovups %%ymm15, (%7)\n" ::"r"(buf + j + k + 0),
            "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24),
            "r"(buf + j + k + 32), "r"(buf + j + k + 40), "r"(buf + j + k + 48),
            "r"(buf + j + k + 56)
            : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8",
              "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
      }
    }
    return;
  }
  if (depth == 8) {
    helper_float_8_recursive(buf + 0, 6);
    helper_float_8_recursive(buf + 64, 6);
    helper_float_8_recursive(buf + 128, 6);
    helper_float_8_recursive(buf + 192, 6);
    for (int j = 0; j < 256; j += 256) {
      for (int k = 0; k < 64; k += 8) {
        __asm__ volatile(
            "vmovups (%0), %%ymm0\n"
            "vmovups (%1), %%ymm1\n"
            "vmovups (%2), %%ymm2\n"
            "vmovups (%3), %%ymm3\n"
            "vaddps %%ymm1, %%ymm0, %%ymm8\n"
            "vsubps %%ymm1, %%ymm0, %%ymm9\n"
            "vaddps %%ymm3, %%ymm2, %%ymm10\n"
            "vsubps %%ymm3, %%ymm2, %%ymm11\n"
            "vaddps %%ymm10, %%ymm8, %%ymm0\n"
            "vsubps %%ymm10, %%ymm8, %%ymm2\n"
            "vaddps %%ymm11, %%ymm9, %%ymm1\n"
            "vsubps %%ymm11, %%ymm9, %%ymm3\n"
            "vmovups %%ymm0, (%0)\n"
            "vmovups %%ymm1, (%1)\n"
            "vmovups %%ymm2, (%2)\n"
            "vmovups %%ymm3, (%3)\n" ::"r"(buf + j + k + 0),
            "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192)
            : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8",
              "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
      }
    }
    return;
  }
#endif
}

inline void helper_float_8(float *buf) { helper_float_8_recursive(buf, 8); }

inline void helper_float_9(float *buf) {
#if defined(__AVX512F__)
  for (int j = 0; j < 512; j += 64) {
    for (int k = 0; k < 8; k += 8) {
      __asm__ volatile(
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $160, %%ymm2, %%ymm8\n"
          "vpermilps $245, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilps $160, %%ymm3, %%ymm8\n"
          "vpermilps $245, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilps $160, %%ymm4, %%ymm8\n"
          "vpermilps $245, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilps $160, %%ymm5, %%ymm8\n"
          "vpermilps $245, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilps $160, %%ymm6, %%ymm8\n"
          "vpermilps $245, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilps $160, %%ymm7, %%ymm8\n"
          "vpermilps $245, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vpermilps $68, %%ymm2, %%ymm8\n"
          "vpermilps $238, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm2\n"
          "vpermilps $68, %%ymm3, %%ymm8\n"
          "vpermilps $238, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm3\n"
          "vpermilps $68, %%ymm4, %%ymm8\n"
          "vpermilps $238, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm4\n"
          "vpermilps $68, %%ymm5, %%ymm8\n"
          "vpermilps $238, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm5\n"
          "vpermilps $68, %%ymm6, %%ymm8\n"
          "vpermilps $238, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm6\n"
          "vpermilps $68, %%ymm7, %%ymm8\n"
          "vpermilps $238, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm7\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm2, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm2\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm3, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm3\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm4, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm4\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm5, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm5\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm6, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm6\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm7, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n" ::"r"(buf + j + k + 0),
          "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32),
          "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56)
          : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8",
            "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
    }
  }
  for (int j = 0; j < 512; j += 512) {
    for (int k = 0; k < 64; k += 8) {
      __asm__ volatile(
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n" ::"r"(buf + j + k + 0),
          "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192),
          "r"(buf + j + k + 256), "r"(buf + j + k + 320), "r"(buf + j + k + 384),
          "r"(buf + j + k + 448)
          : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8",
            "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
    }
  }
#endif
}

inline void helper_float_10(float *buf) {
#if defined(__AVX512F__)
  for (int j = 0; j < 1024; j += 64) {
    for (int k = 0; k < 8; k += 8) {
      __asm__ volatile(
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $160, %%ymm2, %%ymm8\n"
          "vpermilps $245, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilps $160, %%ymm3, %%ymm8\n"
          "vpermilps $245, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilps $160, %%ymm4, %%ymm8\n"
          "vpermilps $245, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilps $160, %%ymm5, %%ymm8\n"
          "vpermilps $245, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilps $160, %%ymm6, %%ymm8\n"
          "vpermilps $245, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilps $160, %%ymm7, %%ymm8\n"
          "vpermilps $245, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vpermilps $68, %%ymm2, %%ymm8\n"
          "vpermilps $238, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm2\n"
          "vpermilps $68, %%ymm3, %%ymm8\n"
          "vpermilps $238, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm3\n"
          "vpermilps $68, %%ymm4, %%ymm8\n"
          "vpermilps $238, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm4\n"
          "vpermilps $68, %%ymm5, %%ymm8\n"
          "vpermilps $238, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm5\n"
          "vpermilps $68, %%ymm6, %%ymm8\n"
          "vpermilps $238, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm6\n"
          "vpermilps $68, %%ymm7, %%ymm8\n"
          "vpermilps $238, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm7\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm2, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm2\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm3, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm3\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm4, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm4\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm5, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm5\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm6, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm6\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm7, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n" ::"r"(buf + j + k + 0),
          "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32),
          "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56)
          : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8",
            "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
    }
  }
  for (int j = 0; j < 1024; j += 512) {
    for (int k = 0; k < 64; k += 8) {
      __asm__ volatile(
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n" ::"r"(buf + j + k + 0),
          "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192),
          "r"(buf + j + k + 256), "r"(buf + j + k + 320), "r"(buf + j + k + 384),
          "r"(buf + j + k + 448)
          : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8",
            "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
    }
  }
  for (int j = 0; j < 1024; j += 1024) {
    for (int k = 0; k < 512; k += 8) {
      __asm__ volatile(
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n" ::"r"(buf + j + k + 0),
          "r"(buf + j + k + 512)
          : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8",
            "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
    }
  }
#endif
}

inline void helper_float_11(float *buf) {
#if defined(__AVX512F__)
  for (int j = 0; j < 2048; j += 64) {
    for (int k = 0; k < 8; k += 8) {
      __asm__ volatile(
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $160, %%ymm2, %%ymm8\n"
          "vpermilps $245, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilps $160, %%ymm3, %%ymm8\n"
          "vpermilps $245, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilps $160, %%ymm4, %%ymm8\n"
          "vpermilps $245, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilps $160, %%ymm5, %%ymm8\n"
          "vpermilps $245, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilps $160, %%ymm6, %%ymm8\n"
          "vpermilps $245, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilps $160, %%ymm7, %%ymm8\n"
          "vpermilps $245, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vpermilps $68, %%ymm2, %%ymm8\n"
          "vpermilps $238, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm2\n"
          "vpermilps $68, %%ymm3, %%ymm8\n"
          "vpermilps $238, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm3\n"
          "vpermilps $68, %%ymm4, %%ymm8\n"
          "vpermilps $238, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm4\n"
          "vpermilps $68, %%ymm5, %%ymm8\n"
          "vpermilps $238, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm5\n"
          "vpermilps $68, %%ymm6, %%ymm8\n"
          "vpermilps $238, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm6\n"
          "vpermilps $68, %%ymm7, %%ymm8\n"
          "vpermilps $238, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm7\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm2, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm2\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm3, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm3\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm4, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm4\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm5, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm5\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm6, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm6\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm7, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n" ::"r"(buf + j + k + 0),
          "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32),
          "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56)
          : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8",
            "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
    }
  }
  for (int j = 0; j < 2048; j += 512) {
    for (int k = 0; k < 64; k += 8) {
      __asm__ volatile(
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n" ::"r"(buf + j + k + 0),
          "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192),
          "r"(buf + j + k + 256), "r"(buf + j + k + 320), "r"(buf + j + k + 384),
          "r"(buf + j + k + 448)
          : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8",
            "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
    }
  }
  for (int j = 0; j < 2048; j += 2048) {
    for (int k = 0; k < 512; k += 8) {
      __asm__ volatile(
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vmovups %%ymm0, (%0)\n"
          "vmovups %%ymm1, (%1)\n"
          "vmovups %%ymm2, (%2)\n"
          "vmovups %%ymm3, (%3)\n" ::"r"(buf + j + k + 0),
          "r"(buf + j + k + 512), "r"(buf + j + k + 1024), "r"(buf + j + k + 1536)
          : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8",
            "%ymm9", "%ymm10", "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
    }
  }
#endif
}
// NOLINTEND
}  // namespace alaya
